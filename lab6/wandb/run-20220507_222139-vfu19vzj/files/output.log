Random Seed:  1
Namespace(batch_size=6, beta=0.0001, beta1=0.9, cond_dim=7, cuda=True, data_root='./processed_data', epoch_size=600, g_dim=128, kl_anneal_cycle=3, kl_anneal_cyclical=False, kl_anneal_ratio=2, last_frame_skip=False, log_dir='./logs/fp/annealcycle = 0 kl_anneal_ratio=2 beta = 0 tfr_start_decay_epoch=40 tfr_decay_step=0', lr=0.001, model_dir='', n_eval=12, n_future=10, n_past=2, niter=300, num_workers=4, optimizer='adam', posterior_rnn_layers=1, predictor_rnn_layers=2, rnn_size=256, seed=1, tfr=1.0, tfr_decay_step=0.01, tfr_lower_bound=0.0, tfr_start_decay_epoch=40, z_dim=64)

  0%|‚ñè                                                          | 1/300 [02:44<13:38:34, 164.26s/it]Traceback (most recent call last):
  File "train_fixed_prior.py", line 398, in <module>
    # plot_pred(validate_x, validate_cond, modules, epoch, args)
  File "train_fixed_prior.py", line 300, in main
    loss, mse, kld = train(x, cond, modules, optimizer, kl_anneal, args)
  File "train_fixed_prior.py", line 98, in train
    z_t, mu, logvar = modules['posterior'](h_target)
  File "/home/leeandy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/leeandy/Deep_learning_lab/lab6/models/lstm.py", line 67, in forward
    embedded = self.embed(input)
  File "/home/leeandy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/leeandy/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/leeandy/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
KeyboardInterrupt
Traceback (most recent call last):
  File "train_fixed_prior.py", line 398, in <module>
    # plot_pred(validate_x, validate_cond, modules, epoch, args)
  File "train_fixed_prior.py", line 300, in main
    loss, mse, kld = train(x, cond, modules, optimizer, kl_anneal, args)
  File "train_fixed_prior.py", line 98, in train
    z_t, mu, logvar = modules['posterior'](h_target)
  File "/home/leeandy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/leeandy/Deep_learning_lab/lab6/models/lstm.py", line 67, in forward
    embedded = self.embed(input)
  File "/home/leeandy/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/leeandy/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/leeandy/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
KeyboardInterrupt